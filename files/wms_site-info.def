##############################################################################
# Copyright (c) Members of the EGEE Collaboration. 2004. 
# See http://www.eu-egee.org/partners/ for details on the copyright 
# holders.  
#
# Licensed under the Apache License, Version 2.0 (the "License"); 
# you may not use this file except in compliance with the License. 
# You may obtain a copy of the License at 
#
#    http://www.apache.org/licenses/LICENSE-2.0 
#
# Unless required by applicable law or agreed to in writing, software 
# distributed under the License is distributed on an "AS IS" BASIS, 
# WITHOUT WARRANTIES OR CONDITIONS 
# OF ANY KIND, either express or implied. 
# See the License for the specific language governing permissions and 
# limitations under the License.
##############################################################################
#
# NAME :        site-info.def
#
# DESCRIPTION : This is the main configuration file needed to execute the
#               yaim command. It contains a list of the variables needed to
#               configure a service.
#
# AUTHORS :     yaim-contact@cern.ch
#
# NOTES :       - site-info.def will contain the list of variables common to 
#                 multiple node types. Node type specific variables are 
#                 distributed by its corresponding module although a unique 
#                 site-info.def can still be used at configuration time. 
#               
#               - Service specific variables are being distributed under 
#                    /opt/glite/yaim/examples/siteinfo/services/<node_type_name>
#                 Copy this file under you siteinfo/services directory or also copy the variables
#                 manually in site-info.def. 
#
#               - site-info.pre and site-info.post contain default variables. When sys admins
#                 want to set their own values, they can just define the variable in site-info.def
#                 and that will overwrite the value in site-info.pre/post.
#
#               - VO variables for LCG VOs are currently distributed with example values. 
#                 For up to date information of any VO please check the CIC portal VO ID Card information:
#                 http://cic.in2p3.fr/ 
#
#               - For more information on YAIM, please check:
#                 https://twiki.cern.ch/twiki/bin/view/EGEE/YAIM
#
#               - For a detailed description of site-info.def variables, please check:
#                 https://twiki.cern.ch/twiki/bin/view/LCG/Site-info_configuration_variables#site_info_def
#   
# YAIM MODULE:  glite-yaim-core
#                 
##############################################################################
 
###################################
# General configuration variables #
###################################

# List of the batch nodes hostnames and optionally the subcluster ID the
# WN belongs to. An example file is available in 
# ${YAIM_ROOT}/glite/yaim/examples/wn-list.conf
# Change the path according to your site settings.
WN_LIST=${YAIM_ROOT}/glite/yaim/etc/wn-list.conf
# Change this to the correct endpoint
ARGUS_PEPD_ENDPOINTS="https://argus.ihep.ac.cn:8154/authz"
# Change this to the correct resource field in the PAP policy
GLEXEC_WN_PEPC_RESOURCEID="http://authz-interop.org/xacml/resource/resource-type/wn"
# Change this to the correct action in the PAP policy
GLEXEC_WN_PEPC_ACTIONID="http://glite.org/xacml/action/execute"

GLEXEC_WN_ARGUS_ENABLED=yes
GLEXEC_WN_SCAS_ENABLED="no"

# Change this to the list of users that may run gLExec 
GLEXEC_EXTRA_WHITELIST=".dteam,.atlas,.ops,.cms,.cmspilot,.atlpilot,.opspilot,.enmr.eu,.argo,.esr"

GLEXEC_WN_OPMODE="setuid"
GLEXEC_WN_USE_LCAS="no"
#GLEXEC_WN_LCAS_DEBUG_LEVEL="0"

GLEXEC_WN_LOG_LEVEL=3
GLEXEC_WN_LCMAPS_DEBUG_LEVEL=3
GLEXEC_WN_LOG_DESTINATION=syslog
#GLEXEC_WN_LOG_DESTINATION=file
#GLEXEC_WN_LOG_FILE=/var/log/glexec/glexec.log
#GLEXEC_WN_LCASLCMAPS_LOG=/var/log/glexec/lcas_lcmaps.log

GLEXEC_WN_INPUT_LOCK=flock
GLEXEC_WN_TARGET_LOCK=flock

# List of unix users to be created in the service nodes.
# The format is as follows:
# UID:LOGIN:GID1,GID2,...:GROUP1,GROUP2,...:VO:FLAG:
# An example file is available in ${YAIM_ROOT}/glite/yaim/examples/users.conf
# Change the path according to your site settings.
# For more information please check ${YAIM_ROOT}/glite/yaim/examples/users.conf.README 
USERS_CONF=${YAIM_ROOT}/glite/yaim/etc/users.conf

EDGUSERS=${YAIM_ROOT}/glite/yaim/etc/edgusers.conf
# List of the local accounts which a user should be mapped to.
# The format is as follows:
# "VOMS_FQAN":GROUP:GID:FLAG:[VO]
# An example file is available in ${YAIM_ROOT}/glite/yaim/examples/groups.conf
# Change the path according to your site settings.
# For more information please check ${YAIM_ROOT}/glite/yaim/examples/groups.conf.README
# NOTE: comment out this variable if you want to specify a groups.conf per VO
# under the group.d/ directory.
GROUPS_CONF=${YAIM_ROOT}/glite/yaim/etc/groups.conf

# Uncomment this variable if you want to specify a local groups.conf 
# It is similar to GROUPS_CONF but used to specify a separate file
# where local accounts specific to the site are defined.
# LOCAL_GROUPS_CONF=my_local_groups.conf

# Uncomment this variable if you are installing a mysql server
# It is the MySQL admin password. 
#MYSQL_PASSWORD=

# Uncomment this variable if you want to explicitely use pool
# accounts for special users when generating the grid-mapfile.
# If not defined, YAIM will decide whether to use special 
# pool accounts or not automatically
# SPECIAL_POOL_ACCOUNTS=yes or no

################################
# Site configuration variables #
################################

# Human-readable name of your site
SITE_NAME=BEIJING-LCG2

# The contact e-mail of your site.
# A coma separated list of email addresses. 
SITE_EMAIL="lcg-admin@ihep.ac.cn"

# It is the position of your site north or south of the equator 
# measured from -90. to 90. with positive values going north and 
# negative values going south. 
SITE_LAT=39.909

# It is the position of the site east or west of Greenwich, England 
# measured from -180. to 180. with positive values going east and 
# negative values going west.  
SITE_LONG=116.241

# Uncomment this variable if your site has an http proxy
# in order to reduce the load on the CA host
# SITE_HTTP_PROXY="http-proxy.my.domain"

#########################################
# ARGUS authorisation framework control #
#########################################

# Set USE_ARGUS to yes to enable the configuration of ARGUS
USE_ARGUS=no

# In case ARGUS is to be used the following should be set
# The ARGUS service PEPD endpoints as a space separated list:
ARGUS_PEPD_ENDPOINTS="https://argus.ihep.ac.cn:8154/authz"
CREAM_PEPC_RESOURCEID="http://emi.ihep.ac.cn/cream"

PAP_ADMIN_DN="/C=CN/O=HEP/O=IHEP/OU=CC/CN=Yan Xiaofei"
ARGUS_HOST=argus.ihep.ac.cn

# ARGUS resource identities: The resource ID can be set
# for the cream CE, WMS and other nodes respectively.
# If a resource ID is left unset the ARGUS configuration
# will be skipped on the associated node.
# CREAM_PEPC_RESOURCEID=urn:mysitename.org:resource:ce
# WMS_PEPC_RESOURCEID=urn:mysitename.org:resource:wms
# GENERAL_PEPC_RESOURCEID=urn:mysitename.org:resource:other

################################
# User configuration variables #
################################

# Uncomment the following variables if you want to create system user
# accounts under a HOME directory different from /home.
# Note: It is recommendable to use /var/lib/user_name as HOME directory for
# system users.
# EDG_HOME_DIR=/var/lib/edguser
# EDGINFO_HOME_DIR=/var/lib/edginfo
# BDII_HOME_DIR=/var/lib/edguser

##############################
# CE configuration variables #
##############################

# Optional variable to define the path of a shared directory 
# available for application data. 
# Typically a POSIX accessible transient disk space shared 
# between the Worker Nodes. It may be used by MPI applications 
# or to store intermediate files that need further processing by 
# local jobs or as staging area, specially if the Worker Node 
# have no internet connectivity 
# CE_DATADIR=/mypath

# Hostname of the CE
CE_HOST=cce.$MY_DOMAIN

############################
# SubCluster configuration #
############################

# Name of the processor model as defined by the vendor 
# for the Worker Nodes in a SubCluster.
CE_CPU_MODEL=X5650

# Name of the processor vendor 
# for the Worker Nodes in a SubCluster
CE_CPU_VENDOR=GenuineIntel

# Processor clock speed expressed in MHz 
# for the Worker Nodes in a SubCluster.
CE_CPU_SPEED=2666

# For the following variables please check:
# http://goc.grid.sinica.edu.tw/gocwiki/How_to_publish_the_OS_name
#
# Operating system name used on the Worker Nodes 
# part of the SubCluster.
CE_OS=ScientificSL

# Operating system release used on the Worker Nodes
# part of the SubCluster.
CE_OS_RELEASE=6.4

# Operating system version used on the Worker Nodes
# part of the SubCluster.
CE_OS_VERSION="Carbon"

# Platform Type of the WN in the SubCluster
# Check: http://goc.grid.sinica.edu.tw/gocwiki/How_to_publish_my_machine_architecture 
CE_OS_ARCH=x86_64

# Total physical memory of a WN in the SubCluster
# expressed in Megabytes.
CE_MINPHYSMEM=16192

# Total virtual memory of a WN in the SubCluster
# expressed in Megabytes.
CE_MINVIRTMEM=16192

# Total number of real CPUs/physical chips in 
# the SubCluster, including the nodes part of the 
# SubCluster that are temporary down or offline. 
CE_PHYSCPU=256

# Total number of cores/hyperthreaded CPUs in 
# the SubCluster, including the nodes part of the 
# SubCluster that are temporary down or offline
CE_LOGCPU=1088

# Number of Logical CPUs (cores) of the WN in the 
# SubCluster
CE_SMPSIZE=8

# Performance index of your fabric in SpecInt 2000
CE_SI00=2732

# Performance index of your fabric in SpecFloat 2000
CE_SF00=2834

# Set this variable to either TRUE or FALSE to express 
# the permission for direct outbound connectivity 
# for the WNs in the SubCluster
CE_OUTBOUNDIP=FALSE

# Set this variable to either TRUE or FALSE to express 
# the permission for inbound connectivity 
# for the WNs in the SubCluster
CE_INBOUNDIP=FALSE

# Space separated list of software tags supported by the site
# e.g. CE_RUNTIMEENV="LCG-2 LCG-2_1_0 LCG-2_1_1 LCG-2_2_0 GLITE-3_0_0 GLITE-3_1_0 R-GMA"
CE_RUNTIMEENV="LCG-2 LCG-2_1_0 LCG-2_1_1 LCG-2_2_0 GLITE-3_0_0 GLITE-3_1_0 R-GMA"

# For the following variables, please check more detailed information in:
# https://twiki.cern.ch/twiki/bin/view/LCG/Site-info_configuration_variables#site_info_def
#
# The following values must be defined by the sys admin:
# - CPUScalingReferenceSI00=<referenceCPU-SI00> 
# - Share=<vo-name>:<vo-share> (optional, multiple definitons) 
#CE_CAPABILITY="CPUScalingReferenceSI00=value [Share=vo-name1:value [Share=vo-name2:value [...]]]"
CE_CAPABILITY="CPUScalingReferenceSI00=2732 [Share=atlas:50 [Share=cms:50 ]]"

# The following values must be defined by the sys admin:
# - Cores=value
# - value-HEP-SPEC06 (mandatory), where "value" is the CPU power computed
#   using the HEP-SPEC06 benchmark
# More information on how to calculate and set this values 
# can be found at https://wiki.egi.eu/wiki/HEP_SPEC06
CE_OTHERDESCR="Cores=4.25[,Benchmark=10.928-HEP-SPEC06]" 


########################################
# Batch server configuration variables #
########################################

# Hostname of the Batch server
# Change this if your batch server is not installed 
# in the same host of the CE
BATCH_SERVER=$CE_HOST

# Jobmanager specific settings. Please, define:
# lcgpbs, lcglfs, lcgsge or lcgcondor
JOB_MANAGER=lcgpbs

# torque, lsf, sge or condor
CE_BATCH_SYS=torque
BATCH_LOG_DIR=/var/torque
BATCH_VERSION=2.5.13-

################################
# APEL configuration variables #
################################

# Database password for the APEL DB.
APEL_DB_PASSWORD="asdfasdfasdfasfai8yi"

###############################
# WMS configuration variables #
###############################

# Hostname of the WMS
WMS_HOST=wms01.$MY_DOMAIN
LB_HOST="wms01.$MY_DOMAIN:9000"
GLITE_LB_TYPE=both
GLITE_LB_AUTHZ_REGISTER_JOBS=".*"
GLITE_LB_WMS_DN="/C=CN/O=HEP/O=IHEP/OU=CC/CN=wms01.ihep.ac.cn"

###################################
# myproxy configuration variables #
###################################

# Hostname of the PX
PX_HOST=lcg006.$MY_DOMAIN

################################
# RGMA configuration variables #
################################

# Hostname of the RGMA server
MON_HOST=lcg008.$MY_DOMAIN

###################################
# FTS configuration variables #
###################################

# FTS endpoint
FTS_SERVER_URL="https://fts.${MY_DOMAIN}:8443/path/glite-data-transfer-fts"

###############################
# DPM configuration variables #
###############################

# Hostname of the DPM head node 
DPM_HOST="ccsrm.$MY_DOMAIN"

########################
# SE general variables #
########################
SE1="srm.ihep.ac.cn"
# Space separated list of SEs hostnames
SE_LIST="DPM_HOST SE1"

# Space separated list of SE hosts from SE_LIST containing 
# the export directory from the Storage Element and the 
# mount directory common to the worker nodes that are part 
# of the Computing Element. If any of the SEs in SE_LIST 
# does not support the mount concept, do not define 
# anything for that SE in this variable. If this is the case 
# for all the SEs in SE_LIST then put the value "none"  
#SE_MOUNT_INFO_LIST="[SE1:export_dir1,mount_dir1 [SE2:export_dir2,mount_dir2 [...]]|none]"
SE_MOUNT_INFO_LIST="sepnfs.ihep.ac.cn:/pnfs"

# Variable necessary to configure Gridview service client on the SEs.
# It sets the location and filename of the gridftp server logfile on 
# different types of SEs. Needed gridftp logfile for gridview is the 
# netlogger file which contain info for each transfer (created with
# -Z/-log-transfer option for globus-gridftp-server). 
# Ex: DATE=20071206082249.108921 HOST=hostname.cern.ch PROG=globus-gridftp-server 
# NL.EVNT=FTP_INFO START=20071206082248.831173 USER=atlas102 FILE=/storage/atlas/ 
# BUFFER=0 BLOCK=262144 NBYTES=330 VOLUME=/ STREAMS=1 STRIPES=1 DEST=[127.0.0.1] 
# TYPE=LIST CODE=226
# Default locations for DPM: /var/log/dpm-gsiftp/dpm-gsiftp.log
#            and SE_classic: /var/log/globus-gridftp.log
SE_GRIDFTP_LOGFILE=/var/log/globus-gridftp.log


################################
# BDII configuration variables #
################################

# Hostname of the top level BDII
BDII_HOST=lcg007.$MY_DOMAIN

# Hostname of the site BDII
SITE_BDII_HOST=lcg002.$MY_DOMAIN

# Uncomment this variable if you want to define a list of
# top level BDIIs to support the automatic failover in the GFAL clients 
# BDII_LIST=my-bdii1.$MY_DOMAIN:port1[,my-bdii22.$MY_DOMAIN:port2[...]] 

##############################
# VO configuration variables #
##############################
# If you are configuring a DNS-like VO, please check
# the following URL: https://twiki.cern.ch/twiki/bin/view/LCG/YaimGuide400#vo_d_directory

# Space separated list of VOs supported by your site
VOS="atlas cms ops dteam bes biomed enmr.eu esr vo.france-asia.org"

# Prefix of the experiment software directory in your CE
VO_SW_DIR=/opt/exp_soft

# Space separated list of queues configured in your CE
QUEUES="atlas cms ops dteam bes biomed enmr.eu esr france_asia.org atlasmcore"

# For each queue defined in QUEUES, define a _GROUP_ENABLE variable 
# which is a space separated list of VO names and VOMS FQANs:
# Ex.: MYQUEUE_GROUP_ENABLE="ops atlas cms /cms/Higgs /cms/ROLE=production"
# In QUEUE names containing dots and dashes replace them with underscore:
# Ex.: QUEUES="my.test-queue"
#      MY_TEST_QUEUE_GROUP_ENABLE="ops atlas"
ATLAS_GROUP_ENABLE="atlas atlaspli atlasprd atlassgm"
CMS_GROUP_ENABLE="cms cmspli cmsprd cmssgm"
OPS_GROUP_ENABLE="ops opspli opsprd opssgm"
DTEAM_GROUP_ENABLE="dteam dteampli dteamprd dtemsgm"
BES_GROUP_ENABLE="bes bespli besprd bessgm"
BIOMED_GROUP_ENABLE="biomed biopli bioprd biosgm"
ENMR_EU_GROUP_ENABLE="enmr.eu enmrpli enmrprd enmrsgm"
ESR_GROUP_ENABLE="esr esrpli esrprd esesgm"
FRANCE_ASIA_ORG_GROUP_ENABLE="france_asia.org vfapli vfaprd vfasgm"
ATLASMCORE_GROUP_ENABLE="atlas atlaspli atlasprd atlassgm"

#############################################
# Cream ce variable                         #
#############################################
MYSQL_PASSWORD=asdfasdfasdfasfai8yi
CREAM_DB_USER=creamdba
CREAM_DB_PASSWORD=asdfasdfasdfasfai8yi
SANDBOX_TRANSFER_METHOD_BETWEEN_CE_WN=LRMS

# Optional variable to define the default SE used by the VO.
# Define the SE hostname if you want a specific SE to be the default one.
# If this variable is not defined, the first SE in SE_LIST will be used
# as the default one.  
# VO_<vo_name>_DEFAULT_SE=vo-default-se

# Optional variable to define a list of LBs used by the VO.
# Define a space separated list of LB hostnames.
# If this variable is not defined LB_HOST will be used.
# VO_<vo_name>_LB_HOSTS="vo-lb1 [vo-lb2 [...]]" 

# Optional variable to automatically add wildcards per FQAN 
# in the LCMAPS gripmap file and groupmap file. Set it to 'yes' 
# if you want to add the wildcards in your VO. Do not define it 
# or set it to 'no' if you do not want to configure wildcards in your VO. 
# VO_<vo_name>_MAP_WILDCARDS=no

# Optional variable to define the Myproxy server supported by the VO. 
# Define the Myproxy hostname if you want a specific Myproxy server. 
# If this variable is not defined PX_HOST will be used. 
# VO_<vo_name>_PX_HOST=vo-myproxy

# Optional variable to define a list of RBs used by the VO.
# Define a space separated list of RB hostnames.
# If this variable is not defined RB_HOST will be used.
# VO_<vo_name>_RBS="vo-rb1 [vo-rb2 [...]]" 

# Area on the WN for the installation of the experiment software. 
# If on the WNs a predefined shared area has been mounted where 
# VO managers can pre-install software, then these variable 
# should point to this area. If instead there is not a shared 
# area and each job must install the software, then this variables 
# should contain a dot ( . ). Anyway the mounting of shared areas, 
# as well as the local installation of VO software is not managed 
# by yaim and should be handled locally by Site Administrators. 
VO_atlas_SW_DIR=/cvmfs/atlas.cern.ch
VO_cms_SW_DIR=/cvmfs/cms.cern.ch

# This variable contains the vomses file parameters needed 
# to contact a VOMS server. Multiple VOMS servers can be given 
# if the parameters are enclosed in single quotes. 
#VO_<vo_name>_VOMSES="'vo_name voms_server_hostname port voms_server_host_cert_dn vo_name' ['...']"

# DN of the CA that signs the VOMS server certificate. 
# Multiple values can be given if enclosed in single quotes. 
# Note that there must be as many entries as in the VO_<vo-name>_VOMSES variable. 
# There is a one to one relationship in the elements of both lists, 
# so the order must be respected
#VO_<vo_name>_VOMS_CA_DN="'voms_server_ca_dn' ['...']"

# A list of the VOMS servers used to create the DN grid-map file. 
# Multiple values can be given if enclosed in single quotes.
#VO_<vo_name>_VOMS_SERVERS="'vomss://<host-name>:8443/voms/<vo-name>?/<vo-name>' ['...']"

# Optional variable to define a list of WMSs used by the VO.
# Define a space separated list of WMS hostnames.
# If this variable is not defined WMS_HOST will be used.
# VO_<vo_name>_WMS_HOSTS="vo-wms1 [vo-wms2 [...]]"

# Optional variable to create a grid-mapfile with mappings to ordinary
# pool accounts, not containing mappings to special users.
# - UNPRIVILEGED_MKGRIDMAP=no or undefined, will contain
# special users if defined in groups.conf
# - UNPRIVILEGED_MKGRIDMAP=yes, will create a grid-mapfile
# containing only mappings to ordinary pool accounts.
# VO_<vo_name>_UNPRIVILEGED_MKGRIDMAP=no

# gLite pool account home directory for the user accounts specified in USERS_CONF.
# Define this variable if you would like to use a directory different than /home.
# VO_<vo_name>_USER_HOME_PREFIX=/pool_account_home_dir

# Examples for the following VOs are included below:
#
# atlas
# alice
# lhcb
# cms
# dteam
# biomed
# ops
#
# VOs should check the CIC portal http://cic.in2p3.fr for the VO ID card information
#
#
#########
# atlas #
#########
 VO_ATLAS_SW_DIR=/cvmfs/atlas.cern.ch/repo/sw
 VO_ATLAS_DEFAULT_SE=ccsrm.ihep.ac.cn
 VO_ATLAS_STORAGE_DIR=/dpm/ihep.ac.cn/home/atlas
VO_ATLAS_VOMSES="\
'cms lcg-voms.cern.ch 15002 \
/DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch cms 24' \
'cms voms.cern.ch 15002 \
/DC=ch/DC=cern/OU=computers/CN=voms.cern.ch cms 24' \
'cms lcg-voms2.cern.ch 15002 \
/DC=ch/DC=cern/OU=computers/CN=lcg-voms2.cern.ch cms 24' \
'cms voms2.cern.ch 15002 \
/DC=ch/DC=cern/OU=computers/CN=voms2.cern.ch cms 24' \
"
VO_ATLAS_VOMS_CA_DN="\
'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' \
'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' \
'/DC=ch/DC=cern/CN=CERN Grid Certification Authority' \
'/DC=ch/DC=cern/CN=CERN Grid Certification Authority' \
"

#
# only 3 of them can be used for making grid-mapfiles
#

VO_CMS_VOMS_SERVERS="\
'vomss://voms.cern.ch:8443/voms/cms?/cms/' \
'vomss://voms2.cern.ch:8443/voms/cms?/cms/' \
'vomss://lcg-voms2.cern.ch:8443/voms/cms?/cms/' \
"

#
##########
# alice  #
##########
# VO_ALICE_SW_DIR=$VO_SW_DIR/alice
# VO_ALICE_DEFAULT_SE=$SE_HOST
# VO_ALICE_STORAGE_DIR=$CLASSIC_STORAGE_DIR/alice
# VO_ALICE_VOMS_SERVERS='vomss://voms.cern.ch:8443/voms/alice?/alice/'
# VO_ALICE_VOMSES="\
# 'alice lcg-voms.cern.ch 15000 \
# /DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch alice 24' \
# 'alice voms.cern.ch 15000 \
# /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch alice 24' \
# "
# VO_ALICE_VOMS_CA_DN="\
# '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' \
# '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' \
# "
#
#######
# cms #
#######
 VO_CMS_SW_DIR=/cvmfs/cms.cern.ch
 VO_CMS_DEFAULT_SE=$SE1
 VO_CMS_STORAGE_DIR=/pnfs/ihep.ac.cn/data/cms
VO_CMS_VOMSES="\
'cms lcg-voms.cern.ch 15002 \
/DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch cms 24' \
'cms voms.cern.ch 15002 \
/DC=ch/DC=cern/OU=computers/CN=voms.cern.ch cms 24' \
'cms lcg-voms2.cern.ch 15002 \
/DC=ch/DC=cern/OU=computers/CN=lcg-voms2.cern.ch cms 24' \
'cms voms2.cern.ch 15002 \
/DC=ch/DC=cern/OU=computers/CN=voms2.cern.ch cms 24' \
"
VO_CMS_VOMS_CA_DN="\
'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' \
'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' \
'/DC=ch/DC=cern/CN=CERN Grid Certification Authority' \
'/DC=ch/DC=cern/CN=CERN Grid Certification Authority' \
"

#
# only 3 of them can be used for making grid-mapfiles
#

VO_CMS_VOMS_SERVERS="\
'vomss://voms.cern.ch:8443/voms/cms?/cms/' \
'vomss://voms2.cern.ch:8443/voms/cms?/cms/' \
'vomss://lcg-voms2.cern.ch:8443/voms/cms?/cms/' \
"
#
########
# lhcb #
########
# VO_LHCB_SW_DIR=$VO_SW_DIR/lhcb
# VO_LHCB_DEFAULT_SE=$SE_HOST
# VO_LHCB_STORAGE_DIR=$CLASSIC_STORAGE_DIR/lhcb
# VO_LHCB_VOMS_SERVERS='vomss://voms.cern.ch:8443/voms/lhcb?/lhcb/'
# VO_LHCB_VOMSES="\
# 'lhcb lcg-voms.cern.ch 15003 \
# /DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch lhcb 24' \
# 'lhcb voms.cern.ch 15003 \
# /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch lhcb 24' \
# "
# VO_LHCB_VOMS_CA_DN="\
# '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' \
# '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' \
# "
#
#########
# dteam #
#########
 VO_DTEAM_SW_DIR=$VO_SW_DIR/dteam
 VO_DTEAM_DEFAULT_SE=$DPM_HOST
 VO_DTEAM_STORAGE_DIR=/dpm/ihep.ac.cn/home/dteam
 VO_DTEAM_VOMS_SERVERS='vomss://voms.hellasgrid.gr:8443/voms/dteam?/dteam/'
 VO_DTEAM_VOMSES="\
 'dteam voms.hellasgrid.gr 15004 \
 /C=GR/O=HellasGrid/OU=hellasgrid.gr/CN=voms.hellasgrid.gr dteam 24' \
 'dteam voms2.hellasgrid.gr 15004 \
 /C=GR/O=HellasGrid/OU=hellasgrid.gr/CN=voms2.hellasgrid.gr dteam 24' \
 "
 VO_DTEAM_VOMS_CA_DN="\
 '/C=GR/O=HellasGrid/OU=Certification Authorities/CN=HellasGrid CA 2006' \
 '/C=GR/O=HellasGrid/OU=Certification Authorities/CN=HellasGrid CA 2006' \
 "

##########
# biomed #
##########
 VO_BIOMED_SW_DIR=$VO_SW_DIR/biomed
 VO_BIOMED_DEFAULT_SE=$SE_HOST
 VO_BIOMED_STORAGE_DIR=/dpm/ihep.ac.cn/home/biomed
 VO_BIOMED_VOMS_SERVERS="vomss://cclcgvomsli01.in2p3.fr:8443/voms/biomed?/biomed/"
 VO_BIOMED_VOMSES="\
 'biomed cclcgvomsli01.in2p3.fr 15000 \
 /O=GRID-FR/C=FR/O=CNRS/OU=CC-IN2P3/CN=cclcgvomsli01.in2p3.fr biomed 24' \
 "
 VO_BIOMED_VOMS_CA_DN="\
 '/C=FR/O=CNRS/CN=GRID2-FR' \
 "
#######
# ops #
#######
 VO_OPS_SW_DIR=$VO_SW_DIR/ops
 VO_OPS_DEFAULT_SE=$DPM_HOST
 VO_OPS_STORAGE_DIR=/dpm/ihep.ac.cn/home/ops
VO_OPS_VOMSES="\
'ops lcg-voms.cern.ch 15009 \
/DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch ops 24' \
'ops voms.cern.ch 15009 \
/DC=ch/DC=cern/OU=computers/CN=voms.cern.ch ops 24' \
'ops lcg-voms2.cern.ch 15009 \
/DC=ch/DC=cern/OU=computers/CN=lcg-voms2.cern.ch ops 24' \
'ops voms2.cern.ch 15009 \
/DC=ch/DC=cern/OU=computers/CN=voms2.cern.ch ops 24' \
"
VO_OPS_VOMS_CA_DN="\
'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' \
'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' \
'/DC=ch/DC=cern/CN=CERN Grid Certification Authority' \
'/DC=ch/DC=cern/CN=CERN Grid Certification Authority' \
"

#
# only 3 of them can be used for making grid-mapfiles
#

VO_OPS_VOMS_SERVERS="\
'vomss://voms.cern.ch:8443/voms/ops?/ops/' \
'vomss://voms2.cern.ch:8443/voms/ops?/ops/' \
'vomss://lcg-voms2.cern.ch:8443/voms/ops?/ops/' \
"
#######
# esr #
######
VO_ESR_SW_DIR=$VO_SW_DIR/esr
VO_ESR_DEFAULT_SE=ccsrm.ihep.ac.cn
VO_ESR_STORAGE_DIR=/dpm/ihep.ac.cn/home/esr

VO_ESR_VOMS_SERVERS="'vomss://voms.grid.sara.nl:8443/voms/esr?/esr' "
VO_ESR_VOMSES="'esr voms.grid.sara.nl 30001 /O=dutchgrid/O=hosts/OU=sara.nl/CN=voms.grid.sara.nl esr' "
VO_ESR_VOMS_CA_DN="'/C=NL/O=NIKHEF/CN=NIKHEF medium-security certification auth' "

#######
# bes #
#######
 VO_BES_SW_DIR=/cvmfs/bes.cern.ch
 VO_BES_DEFAULT_SE=bes-srm.ihep.ac.cn
 VO_BES_STORAGE_DIR=/dcache/bes
 VO_BES_VOMS_SERVERS="vomss://voms.ihep.ac.cn:8443/voms/bes?/bes/"
 VO_BES_VOMSES="\
 'bes voms.ihep.ac.cn 15001 \
 /C=CN/O=HEP/OU=CC/O=IHEP/CN=voms.ihep.ac.cn bes 24' \
 "
 VO_BES_VOMS_CA_DN="\
 '/C=CN/O=HEP/CN=Institute of High Energy Physics Certification Authority' \
 "
######################
# vo.france-asia.org #
######################

 VO_VO_FRANCE_ASIA_ORG_SW_DIR=$VO_SW_DIR/vo.france-asia.org
 VO_VO_FRANCE_ASIA_ORG_DEFAULT_SE=$DPM_HOST
 VO_VO_FRANCE_ASIA_ORG_STORAGE_DIR=/dpm/ihep.ac.cn/home/vo.france-asia.org
 VO_VO_FRANCE_ASIA_ORG_VOMS_SERVERS="vomss://cclcgvomsli01.in2p3.fr:8443/voms/biomed?/biomed/"
 VO_VO_FRANCE_ASIA_ORG_VOMSES="\
 'vo.france-asia.org cclcgvomsli01.in2p3.fr 15019 \
 /O=GRID-FR/C=FR/O=CNRS/OU=CC-IN2P3/CN=cclcgvomsli01.in2p3.fr vo.france-asia.org 24' \
 "
 VO_VO_FRANCE_ASIA_ORG_VOMS_CA_DN="\
 '/C=FR/O=CNRS/CN=GRID2-FR' \
 "
###########
# enmr.eu #
###########
VO_ENMR_EU_SW_DIR=/cvmfs/wenmr.egi.eu
VO_ENMR_EU_DEFAULT_SE=$DPM_HOST
VO_ENMR_EU_STORAGE_DIR=/dpm/ihep.ac.cn/home/enmr
VO_ENMR_EU_VOMS_SERVERS="'vomss://voms2.cnaf.infn.it:8443/voms/enmr.eu?/enmr.eu' 'vomss://voms-02.pd.infn.it:8443/voms/enmr.eu?/enmr.eu'"
VO_ENMR_EU_VOMSES="'enmr.eu voms2.cnaf.infn.it 15014 /C=IT/O=INFN/OU=Host/L=CNAF/CN=voms2.cnaf.infn.it enmr.eu' 'enmr.eu voms-02.pd.infn.it 15014 /C=IT/O=INFN/OU=Host/L=Padova/CN=voms-02.pd.infn.it enmr.eu'"
VO_ENMR_EU_VOMS_CA_DN="'/C=IT/O=INFN/CN=INFN CA' '/C=IT/O=INFN/CN=INFN CA'"
